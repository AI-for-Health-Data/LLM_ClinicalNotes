{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a59d497-6510-4d5a-bf29-6d41a788e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1 - Simple Prompting (Chunking - To avoid hallunication)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "login(token=\"HuggingFaceToken\")  # Replace with your actual token\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Step 3: Load and prepare data\n",
    "df = pd.read_csv(\"filtered_unstructured.csv\", low_memory=False)\n",
    "df = df.iloc[10:20] \n",
    "\n",
    "# Step 4: Identify note chunk columns\n",
    "note_chunk_cols = [col for col in df.columns if col.startswith(\"note_chunk\")]\n",
    "\n",
    "# Combine only non-empty note chunks per row\n",
    "def combine_chunks(row):\n",
    "    return \" \".join([str(row[col]).strip() for col in note_chunk_cols if pd.notna(row[col]) and str(row[col]).strip() != \"\"])\n",
    "\n",
    "df[\"full_note\"] = df.apply(combine_chunks, axis=1)\n",
    "\n",
    "# Step 5: Paraphrasing function\n",
    "\n",
    "def split_chunks(text, chunk_size=400, overlap=100):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def paraphrase(note):\n",
    "    prompt = f\"\"\"Rewrite the following clinical note in a more concise and clear way. Keep all important medical details like diagnoses, treatments, medications, history, and vital signs.\n",
    "\n",
    "Original Note:\n",
    "{note}\n",
    "\n",
    "Paraphrased Note:\"\"\"\n",
    "\n",
    "    # Tokenize and move to model device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode output\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Try to extract only the paraphrased note\n",
    "    if \"Paraphrased Note:\" in decoded:\n",
    "        return decoded.split(\"Paraphrased Note:\")[-1].strip()\n",
    "    elif \"Paraphrased:\" in decoded:\n",
    "        return decoded.split(\"Paraphrased:\")[-1].strip()\n",
    "    elif note.strip() in decoded:\n",
    "        return decoded.replace(note.strip(), \"\").strip()\n",
    "    else:\n",
    "        return decoded.strip()\n",
    "\n",
    "\n",
    "def paraphrase_long_note(note):\n",
    "    chunks = split_chunks(note)\n",
    "    paraphrased_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():  # avoid empty inputs\n",
    "            paraphrased = paraphrase(chunk)\n",
    "            paraphrased_chunks.append(paraphrased)\n",
    "\n",
    "    return \" \".join(paraphrased_chunks).strip()\n",
    "\n",
    "# Apply paraphrasing with chunking to each note\n",
    "tqdm.pandas()\n",
    "df[\"paraphrased_note\"] = df[\"full_note\"].progress_apply(paraphrase_long_note)\n",
    "\n",
    "# Step 6: Apply paraphrasing\n",
    "#df[\"paraphrased_note\"] = df[\"full_note\"].progress_apply(paraphrase)\n",
    "\n",
    "# Step 7: Save results\n",
    "df.to_csv(\"paraphrased_notes_output.csv\", index=False)\n",
    "print(\"Paraphrased notes saved to 'paraphrased_notes_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3db9c1-a59a-4ec3-8f20-2204c9d713b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paraphrased_note\"].to_csv('para_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28abb58b-cd7f-4e25-81dc-6230b8a492d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2: Sentence tokenization + Chunking\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Login to Hugging Face\n",
    "login(token=\"Token\")  # Replace with your actual token\n",
    "\n",
    "# Step 2: Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Step 3: Load and prepare data (ONE NOTE ONLY)\n",
    "df = pd.read_csv(\"filtered_unstructured.csv\", low_memory=False)\n",
    "note_chunk_cols = [col for col in df.columns if col.startswith(\"note_chunk\")]\n",
    "row = df.iloc[11]\n",
    "\n",
    "# Combine non-empty chunks\n",
    "full_note = \" \".join([str(row[col]).strip() for col in note_chunk_cols if pd.notna(row[col]) and str(row[col]).strip() != \"\"])\n",
    "\n",
    "# Step 4: Regex-based sentence-aware chunking\n",
    "def regex_sentence_split(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "def split_chunks_by_sentence(text, chunk_token_limit=350, overlap_sentences=2):\n",
    "    sentences = regex_sentence_split(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_token_len = len(tokenizer.encode(sent, add_special_tokens=False))\n",
    "        if current_length + sent_token_len > chunk_token_limit:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = current_chunk[-overlap_sentences:] + [sent]\n",
    "                current_length = sum(len(tokenizer.encode(s, add_special_tokens=False)) for s in current_chunk)\n",
    "            else:\n",
    "                chunks.append(sent)\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        else:\n",
    "            current_chunk.append(sent)\n",
    "            current_length += sent_token_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Step 5: Paraphrasing function with medical factuality preserving prompt\n",
    "def paraphrase(note):\n",
    "    prompt = f\"\"\"<s>[INST] You are a clinical documentation assistant. \n",
    "    Rephrase the following clinical note to make it clearer, but do not add, change, or remove any medical information.\n",
    "    Keep exact medication names, dosages, and frequencies if present. Do not infer or summarize anything that is not explicitly stated.\n",
    "\n",
    "\n",
    "{note}\n",
    "\n",
    "Rephrased Note: [/INST]\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,  # Reduced to discourage creativity\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove prompt and everything before the response\n",
    "    if \"[/INST]\" in decoded:\n",
    "        return decoded.split(\"[/INST]\")[-1].strip()\n",
    "    else:\n",
    "        return decoded.strip()\n",
    "\n",
    "\n",
    "# Step 6: Split and paraphrase\n",
    "chunks = split_chunks_by_sentence(full_note)\n",
    "\n",
    "print(\"\\n=== ORIGINAL vs PARAPHRASED CHUNKS ===\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if chunk.strip():\n",
    "        paraphrased = paraphrase(chunk)\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Original:\\n{chunk.strip()}\\n\")\n",
    "        print(f\"Paraphrased:\\n{paraphrased}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b531ba0-76c0-4939-8b4f-a3717a69c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
